{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ece9f-b191-499d-aa27-28b6fb3b6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39cd6d4d-36e0-4bcc-88c9-2d3c5ea13bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "import skimage\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e42983-79b8-4239-b4bf-a7468d75718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "\n",
    "  @staticmethod\n",
    "  def parameter(start, end, delta=0):\n",
    "\n",
    "    value = np.random.randint(start-delta, end+delta)\n",
    "    parameters = len(range(start-delta, end+delta))\n",
    "\n",
    "    return value, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecc8446-2fe9-46cd-b214-ac15df44a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_MIN = 20\n",
    "DELTA_MAX = 80\n",
    "SIZE = (100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89a3998-53b9-49cf-965c-80f89a0744df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle(flags=[False,False], preset=None):\n",
    "\n",
    "\n",
    "    var_y = flags[0]\n",
    "    var_x = flags[1]\n",
    "\n",
    "\n",
    "    sparse = None\n",
    "    image = None\n",
    "    label = None\n",
    "    parameters = 1\n",
    "\n",
    "\n",
    "    Y_RANGE = (DELTA_MIN, DELTA_MAX)\n",
    "    X_RANGE = (DELTA_MIN, DELTA_MAX)\n",
    "\n",
    "    DOF = 90\n",
    "    ANGLE = np.random.randint(1, DOF+1)\n",
    "    parameters *= DOF\n",
    "\n",
    "    if preset:\n",
    "      ANGLE=preset\n",
    "\n",
    "    LENGTH = DELTA_MIN\n",
    "\n",
    "    X = int(SIZE[1] / 2)\n",
    "    if var_x:\n",
    "      X, p = Util.parameter(X_RANGE[0], X_RANGE[1])\n",
    "      parameters *= p\n",
    "\n",
    "    Y = int(SIZE[0] / 2)\n",
    "    if var_y:\n",
    "      Y, p = Util.parameter(Y_RANGE[0], Y_RANGE[1])\n",
    "      parameters *= p\n",
    "\n",
    "    image = np.zeros(SIZE, dtype=bool)\n",
    "\n",
    "    # first line\n",
    "    first_angle = np.random.randint(360)\n",
    "    theta = -(np.pi / 180.0) * first_angle\n",
    "    END = (Y - LENGTH * np.cos(theta), X - LENGTH * np.sin(theta))\n",
    "    rr, cc = skimage.draw.line(Y, X, int(np.round(END[0])), int(np.round(END[1])))\n",
    "    image[rr, cc] = 1\n",
    "\n",
    "    second_angle = first_angle+ANGLE\n",
    "    theta = -(np.pi / 180.0) * second_angle\n",
    "    END = (Y - LENGTH * np.cos(theta), X - LENGTH * np.sin(theta))\n",
    "    rr, cc = skimage.draw.line(Y, X, int(np.round(END[0])), int(np.round(END[1])))\n",
    "    image[rr, cc] = 1\n",
    "\n",
    "    sparse = [Y, X, ANGLE, first_angle]\n",
    "\n",
    "    label = ANGLE\n",
    "\n",
    "    return sparse, image, label, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70543f58-6a98-4075-912e-18208dce7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(flags=[False, False, False], preset=None):\n",
    "\n",
    "    var_y = flags[0]\n",
    "    var_x = flags[1]\n",
    "    var_width = flags[2]\n",
    "\n",
    "\n",
    "    sparse = None\n",
    "    image = None\n",
    "    label = None\n",
    "    parameters = 1\n",
    "\n",
    "    Y_RANGE = (DELTA_MIN, DELTA_MAX)\n",
    "    X_RANGE = (DELTA_MIN, DELTA_MAX)\n",
    "\n",
    "    LENGTH, p = Util.parameter(1, Y_RANGE[1]-Y_RANGE[0]+1) # 1..60\n",
    "    parameters *= p\n",
    "\n",
    "    if preset:\n",
    "      LENGTH = preset\n",
    "\n",
    "    MAX_LENGTH = Y_RANGE[1]-Y_RANGE[0]\n",
    "    # print 'Max length', MAX_LENGTH\n",
    "\n",
    "    X = math.floor(SIZE[1] / 2)\n",
    "    if var_x:\n",
    "      X, p = Util.parameter(X_RANGE[0], X_RANGE[1])\n",
    "      parameters *= p\n",
    "\n",
    "    Y = Y_RANGE[0]\n",
    "    if var_y:\n",
    "      \n",
    "      Y, p = Util.parameter(0, SIZE[0]-MAX_LENGTH)\n",
    "      # print 'Y',Y\n",
    "      parameters *= p\n",
    "\n",
    "    WIDTH = 1\n",
    "    if var_width:\n",
    "      sizes = [1, 3, 5, 7, 9, 11]\n",
    "      WIDTH = np.random.choice(sizes)\n",
    "      parameters *= len(sizes)\n",
    "\n",
    "    sparse = [Y, X, LENGTH, WIDTH]\n",
    "\n",
    "    image = np.zeros(SIZE, dtype=bool)\n",
    "\n",
    "\n",
    "    half_width = math.floor(WIDTH / 2) # this always floors\n",
    "    \n",
    "    # print(Y,LENGTH,X,half_width,WIDTH)\n",
    "    image[Y:Y+LENGTH, X-half_width:X+half_width+1] = 1\n",
    "\n",
    "\n",
    "    label = LENGTH\n",
    "\n",
    "    return sparse, image, label, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b0f92ba-fb94-48b3-9108-c9fd69fb732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save(dataset, output_folder, subset_name):\n",
    "    # Define image subfolder within output folder\n",
    "    subset_folder = os.path.join(output_folder, subset_name)\n",
    "    image_subfolder = os.path.join(output_folder, 'images')\n",
    "\n",
    "\n",
    "    if not os.path.exists(image_subfolder):\n",
    "        os.makedirs(image_subfolder)\n",
    "\n",
    "\n",
    "    if not os.path.exists(subset_folder):\n",
    "        os.makedirs(subset_folder)\n",
    "\n",
    "\n",
    "    # Initialize list to hold all JSON data\n",
    "    json_data_list = []\n",
    "\n",
    "\n",
    "    # Process and save images and labels\n",
    "    for item in dataset:\n",
    "        # Load image if it's a URL or a file path\n",
    "        if isinstance(item['image'], str):\n",
    "            response = requests.get(item['image'])\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "        else:\n",
    "            image = item['image']  # Assuming it's a PIL.Image object\n",
    "\n",
    "\n",
    "        # Create a unique ID for each image\n",
    "        unique_id = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "        # Define image path\n",
    "        image_path = os.path.join(image_subfolder, f\"{unique_id}.jpg\")\n",
    "\n",
    "\n",
    "        # Save image\n",
    "        image.save(image_path)\n",
    "\n",
    "\n",
    "        # Remove duplicates and format answers\n",
    "        answers = item['answers']\n",
    "        unique_answers = list(set(answers))\n",
    "        formatted_answers = \", \".join(unique_answers)\n",
    "\n",
    "\n",
    "        # Structure for LLaVA JSON\n",
    "        json_data = {\n",
    "            \"id\": unique_id,\n",
    "            \"image\": f\"{unique_id}.jpg\",\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": item['question']\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": formatted_answers\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "        # Append to list\n",
    "        json_data_list.append(json_data)\n",
    "\n",
    "\n",
    "    # Save the JSON data list to a file\n",
    "    json_output_path = os.path.join(output_folder, subset_name, 'dataset.json')\n",
    "    with open(json_output_path, 'w') as json_file:\n",
    "        json.dump(json_data_list, json_file, indent=4)\n",
    "\n",
    "\n",
    "def save_dataset(dataset_name, output_folder, class_name, subset_name, val_samples=None):\n",
    "    # Load the dataset from Hugging Face\n",
    "    dataset = load_dataset(dataset_name, split=subset_name)\n",
    "\n",
    "\n",
    "    # Filter for images with the specified class in 'question_type'\n",
    "    filtered_dataset = [item for item in dataset if item['question_type'] == class_name]\n",
    "\n",
    "\n",
    "    # Determine the split for training and validation\n",
    "    if val_samples is not None and subset_name == 'train':\n",
    "        train_dataset = filtered_dataset[val_samples:]\n",
    "        val_dataset = filtered_dataset[:val_samples]\n",
    "    else:\n",
    "        train_dataset = filtered_dataset\n",
    "        val_dataset = []\n",
    "\n",
    "\n",
    "    # Process and save the datasets\n",
    "    for subset, data in [('train', train_dataset), ('validation', val_dataset)]:\n",
    "        if data:\n",
    "            process_and_save(data, output_folder, subset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ccbb54-e247-4676-ba6e-e73085847ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'dataset'\n",
    "class_name = 'other'\n",
    "val_samples = 300\n",
    "save_dataset('Multimodal-Fatima/OK-VQA_train', output_folder, class_name, 'train', val_samples)\n",
    "save_dataset('Multimodal-Fatima/OK-VQA_test', output_folder, class_name, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1436c6dd-cf4b-42ae-99d6-239be9e4a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0481a63-d548-41b3-907f-59eaab2465e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad0621ae-8027-4aea-8932-cc49ddee782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_save_dataset(output_folder, subset_name, number_of_data):\n",
    "    \n",
    "    subset_folder = os.path.join(output_folder, subset_name)\n",
    "    image_subfolder = os.path.join(output_folder, 'images')\n",
    "\n",
    "\n",
    "    if not os.path.exists(image_subfolder):\n",
    "        os.makedirs(image_subfolder)\n",
    "\n",
    "\n",
    "    if not os.path.exists(subset_folder):\n",
    "        os.makedirs(subset_folder)\n",
    "\n",
    "\n",
    "    # Initialize list to hold all JSON data\n",
    "    json_data_list = []\n",
    "\n",
    "    for i in range(number_of_data):\n",
    "        sparse, image, answer, parameters = length()\n",
    "        int_array = image.astype(int)\n",
    "        uint8_array = (int_array * 255).astype(np.uint8) \n",
    "        \n",
    "        img = Image.fromarray(uint8_array)\n",
    "        \n",
    "        unique_id = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "        # Define image path\n",
    "        image_path = os.path.join(image_subfolder, f\"{unique_id}.jpg\")\n",
    "\n",
    "\n",
    "        # Save image\n",
    "        img.save(image_path)\n",
    "\n",
    "        json_data = {\n",
    "            \"id\": unique_id,\n",
    "            \"image\": f\"{unique_id}.jpg\",\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": 'What is the length of the line in this picture?'\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": str(answer)\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "        # Append to list\n",
    "        json_data_list.append(json_data)\n",
    "\n",
    "        json_output_path = os.path.join(output_folder, subset_name, 'dataset.json')\n",
    "        with open(json_output_path, 'w') as json_file:\n",
    "            json.dump(json_data_list, json_file, indent=4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d586f12c-5c55-40d1-bd6a-fe9759099608",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'lengthDataset'\n",
    "number_of_data = 300\n",
    "\n",
    "generate_save_dataset(output_folder, 'train', number_of_data)\n",
    "generate_save_dataset(output_folder, 'validation', number_of_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9781380-1cce-4cd0-abfa-4451f53abf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_save_dataset(output_folder, 'test', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a725803-76c9-4ce8-a2d7-847a5cbbf44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-07 18:27:34,391] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10147fa209314c44b06b69a6d0301c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path),\n",
    "    offload_folder=\"/content/llava_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7210db91-56b6-499f-9174-98e4b0420acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"LLaVA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2be2fc9-4b8f-431f-8404-42c1b8c20bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahsa-geshvadi001\u001b[0m (\u001b[33mmpsych\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/mahsa.geshvadi001/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mahsa.geshvadi001/Projects/LLaVA/wandb/run-20240307_184655-0zb656jw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mpsych/LLaVA/runs/0zb656jw' target=\"_blank\">northern-violet-1</a></strong> to <a href='https://wandb.ai/mpsych/LLaVA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mpsych/LLaVA' target=\"_blank\">https://wandb.ai/mpsych/LLaVA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mpsych/LLaVA/runs/0zb656jw' target=\"_blank\">https://wandb.ai/mpsych/LLaVA/runs/0zb656jw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mpsych/LLaVA/runs/0zb656jw?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f897dc557d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"*******************************\")\n",
    "wandb.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5adeb-f1a5-42d6-abd7-18014bc5ac06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "113447e1-5501-4f3f-8e64-47eb320df47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign paths to variables\n",
    "DEEPSPEED_SCRIPT = \"deepspeed llava/train/train_mem.py\"\n",
    "DEEPSPEED_JSON = \"./scripts/zero3.json\"\n",
    "MODEL_NAME = \"liuhaotian/llava-v1.5-7b\"\n",
    "DATA_PATH = \"lengthDataset/train/dataset.json\"  # Replace with your JSON data path\n",
    "IMAGE_FOLDER = \"lengthDataset/images\"  # Replace with your image folder path\n",
    "VISION_TOWER = \"openai/clip-vit-large-patch14-336\"\n",
    "OUTPUT_DIR = \"lengthDataset/\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e12dcf5-69e6-4677-8e1e-be3e131ce854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to run the script\n",
    "finetune_script = f'''\n",
    "{DEEPSPEED_SCRIPT} \\\n",
    "    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\n",
    "    --deepspeed {DEEPSPEED_JSON} \\\n",
    "    --model_name_or_path {MODEL_NAME} \\\n",
    "    --version v1 \\\n",
    "    --data_path {DATA_PATH} \\\n",
    "    --image_folder {IMAGE_FOLDER} \\\n",
    "    --vision_tower {VISION_TOWER} \\\n",
    "    --mm_projector_type mlp2x_gelu \\\n",
    "    --mm_vision_select_layer -2 \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --image_aspect_ratio pad \\\n",
    "    --group_by_modality_length True \\\n",
    "    --bf16 True \\\n",
    "    --output_dir {OUTPUT_DIR} \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 50000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --lazy_preprocess True \\\n",
    "    --report_to wandb\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ea13c72-a8ce-407b-a202-606e98f48e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be84bd0a-241b-4b71-bee3-51d83ed4a5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-07 19:04:41,503] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 19:04:44,182] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0\n",
      "[2024-03-07 19:04:44,183] [INFO] [runner.py:568:main] cmd = /home/mahsa.geshvadi001/miniconda3/envs/LLMP/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path liuhaotian/llava-v1.5-7b --version v1 --data_path lengthDataset/train/dataset.json --image_folder lengthDataset/images --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir lengthDataset/ --num_train_epochs 5 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb\n",
      "[2024-03-07 19:04:45,783] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 19:04:47,565] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2024-03-07 19:04:47,566] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2024-03-07 19:04:47,566] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2024-03-07 19:04:47,566] [INFO] [launch.py:163:main] dist_world_size=1\n",
      "[2024-03-07 19:04:47,566] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "[2024-03-07 19:04:47,567] [INFO] [launch.py:253:main] process 1979528 spawned with command: ['/home/mahsa.geshvadi001/miniconda3/envs/LLMP/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--version', 'v1', '--data_path', 'lengthDataset/train/dataset.json', '--image_folder', 'lengthDataset/images', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', 'lengthDataset/', '--num_train_epochs', '5', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']\n",
      "[2024-03-07 19:04:49,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-07 19:04:52,927] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-03-07 19:04:52,927] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "[2024-03-07 19:04:54,800] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 295, num_elems = 6.76B\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:12<00:00,  6.12s/it]\n",
      "Adding LoRA adapters...\n",
      "[2024-03-07 19:06:15,265] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 686, num_elems = 7.06B\n",
      "Formatting inputs...Skip in lazy mode\n",
      "/home/mahsa.geshvadi001/miniconda3/envs/LLMP/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/home/mahsa.geshvadi001/miniconda3/envs/LLMP/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Parameter Offload: Total persistent parameters: 599040 in 312 params\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahsa-geshvadi001\u001b[0m (\u001b[33mmpsych\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/mahsa.geshvadi001/Projects/LLaVA/wandb/run-20240307_190618-mz1134pz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfearless-dew-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mpsych/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mpsych/huggingface/runs/mz1134pz\u001b[0m\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]/home/mahsa.geshvadi001/miniconda3/envs/LLMP/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/mahsa.geshvadi001/miniconda3/envs/LLMP/lib/python3.11/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "[2024-03-07 19:06:32,913] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "{'loss': 4.3614, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.05}         \n",
      "{'loss': 4.3682, 'learning_rate': 0.00013333333333333334, 'epoch': 0.11}        \n",
      "{'loss': 1.9316, 'learning_rate': 0.0002, 'epoch': 0.16}                        \n",
      "{'loss': 1.2438, 'learning_rate': 0.0001999417022366174, 'epoch': 0.21}         \n",
      "{'loss': 1.2657, 'learning_rate': 0.00019976687691905393, 'epoch': 0.26}        \n",
      "{'loss': 1.1133, 'learning_rate': 0.00019947572788580947, 'epoch': 0.32}        \n",
      "{'loss': 1.2225, 'learning_rate': 0.00019906859460363307, 'epoch': 0.37}        \n",
      "{'loss': 1.1457, 'learning_rate': 0.00019854595177171968, 'epoch': 0.42}        \n",
      "{'loss': 1.1989, 'learning_rate': 0.00019790840876823232, 'epoch': 0.47}        \n",
      "{'loss': 1.1588, 'learning_rate': 0.00019715670893979414, 'epoch': 0.53}        \n",
      "{'loss': 1.1248, 'learning_rate': 0.00019629172873477995, 'epoch': 0.58}        \n",
      "{'loss': 1.0746, 'learning_rate': 0.00019531447668141608, 'epoch': 0.63}        \n",
      "{'loss': 1.0582, 'learning_rate': 0.00019422609221188207, 'epoch': 0.68}        \n",
      "{'loss': 1.1687, 'learning_rate': 0.0001930278443337833, 'epoch': 0.74}         \n",
      "{'loss': 1.0803, 'learning_rate': 0.00019172113015054532, 'epoch': 0.79}        \n",
      "{'loss': 1.0629, 'learning_rate': 0.00019030747323245327, 'epoch': 0.84}        \n",
      "{'loss': 1.1086, 'learning_rate': 0.0001887885218402375, 'epoch': 0.89}         \n",
      "{'loss': 1.112, 'learning_rate': 0.00018716604700327514, 'epoch': 0.95}         \n",
      " 19%|████████▏                                  | 18/95 [00:32<01:27,  1.14s/it]Invalidate trace cache @ step 315: expected module 4, but got module 3\n",
      "{'loss': 1.0552, 'learning_rate': 0.00018544194045464886, 'epoch': 1.0}         \n",
      "{'loss': 1.0462, 'learning_rate': 0.0001836182124254711, 'epoch': 1.05}         \n",
      "{'loss': 1.0965, 'learning_rate': 0.0001816969893010442, 'epoch': 1.11}         \n",
      "{'loss': 1.0004, 'learning_rate': 0.00017968051114159047, 'epoch': 1.16}        \n",
      "{'loss': 1.1863, 'learning_rate': 0.000177571129070442, 'epoch': 1.21}          \n",
      "{'loss': 1.048, 'learning_rate': 0.00017537130253273613, 'epoch': 1.26}         \n",
      "{'loss': 1.0722, 'learning_rate': 0.00017308359642781242, 'epoch': 1.32}        \n",
      "{'loss': 1.0595, 'learning_rate': 0.00017071067811865476, 'epoch': 1.37}        \n",
      "{'loss': 1.1331, 'learning_rate': 0.00016825531432186543, 'epoch': 1.42}        \n",
      "{'loss': 1.1107, 'learning_rate': 0.00016572036788179727, 'epoch': 1.47}        \n",
      "{'loss': 1.1246, 'learning_rate': 0.00016310879443260528, 'epoch': 1.53}        \n",
      "{'loss': 1.057, 'learning_rate': 0.00016042363895210946, 'epoch': 1.58}         \n",
      "{'loss': 1.0559, 'learning_rate': 0.00015766803221148673, 'epoch': 1.63}        \n",
      "{'loss': 1.0624, 'learning_rate': 0.00015484518712493187, 'epoch': 1.68}        \n",
      "{'loss': 1.0941, 'learning_rate': 0.00015195839500354335, 'epoch': 1.74}        \n",
      "{'loss': 1.0866, 'learning_rate': 0.00014901102171780174, 'epoch': 1.79}        \n",
      "{'loss': 1.1136, 'learning_rate': 0.00014600650377311522, 'epoch': 1.84}        \n",
      "{'loss': 1.0522, 'learning_rate': 0.0001429483443030082, 'epoch': 1.89}         \n",
      "{'loss': 1.1116, 'learning_rate': 0.00013984010898462416, 'epoch': 1.95}        \n",
      " 39%|████████████████▋                          | 37/95 [00:57<01:08,  1.18s/it]Invalidate trace cache @ step 315: expected module 4, but got module 3\n",
      "{'loss': 1.0807, 'learning_rate': 0.00013668542188130566, 'epoch': 2.0}         \n",
      "{'loss': 1.0553, 'learning_rate': 0.00013348796121709862, 'epoch': 2.05}        \n",
      "{'loss': 1.0073, 'learning_rate': 0.0001302514550881076, 'epoch': 2.11}         \n",
      "{'loss': 0.9986, 'learning_rate': 0.00012697967711570242, 'epoch': 2.16}        \n",
      "{'loss': 1.1026, 'learning_rate': 0.00012367644204664468, 'epoch': 2.21}        \n",
      "{'loss': 1.0159, 'learning_rate': 0.0001203456013052634, 'epoch': 2.26}         \n",
      "{'loss': 1.141, 'learning_rate': 0.00011699103850286669, 'epoch': 2.32}         \n",
      "{'loss': 1.0887, 'learning_rate': 0.00011361666490962468, 'epoch': 2.37}        \n",
      "{'loss': 1.0285, 'learning_rate': 0.00011022641489420342, 'epoch': 2.42}        \n",
      "{'loss': 1.0807, 'learning_rate': 0.0001068242413364671, 'epoch': 2.47}         \n",
      "{'loss': 1.087, 'learning_rate': 0.00010341411101859679, 'epoch': 2.53}         \n",
      "{'loss': 1.0642, 'learning_rate': 0.0001, 'epoch': 2.58}                        \n",
      "{'loss': 1.0195, 'learning_rate': 9.658588898140322e-05, 'epoch': 2.63}         \n",
      "{'loss': 1.1867, 'learning_rate': 9.317575866353292e-05, 'epoch': 2.68}         \n",
      "{'loss': 1.0566, 'learning_rate': 8.977358510579657e-05, 'epoch': 2.74}         \n",
      "{'loss': 1.0893, 'learning_rate': 8.638333509037536e-05, 'epoch': 2.79}         \n",
      "{'loss': 1.0687, 'learning_rate': 8.300896149713334e-05, 'epoch': 2.84}         \n",
      "{'loss': 1.1018, 'learning_rate': 7.965439869473664e-05, 'epoch': 2.89}         \n",
      "{'loss': 1.1038, 'learning_rate': 7.632355795335533e-05, 'epoch': 2.95}         \n",
      " 59%|█████████████████████████▎                 | 56/95 [01:22<00:45,  1.17s/it]Invalidate trace cache @ step 315: expected module 4, but got module 3\n",
      "{'loss': 1.0586, 'learning_rate': 7.302032288429756e-05, 'epoch': 3.0}          \n",
      "{'loss': 1.0333, 'learning_rate': 6.974854491189243e-05, 'epoch': 3.05}         \n",
      "{'loss': 1.0611, 'learning_rate': 6.651203878290139e-05, 'epoch': 3.11}         \n",
      "{'loss': 1.0675, 'learning_rate': 6.331457811869437e-05, 'epoch': 3.16}         \n",
      "{'loss': 1.035, 'learning_rate': 6.015989101537586e-05, 'epoch': 3.21}          \n",
      "{'loss': 1.076, 'learning_rate': 5.7051655696991826e-05, 'epoch': 3.26}         \n",
      "{'loss': 1.0325, 'learning_rate': 5.399349622688479e-05, 'epoch': 3.32}         \n",
      "{'loss': 1.0362, 'learning_rate': 5.0988978282198305e-05, 'epoch': 3.37}        \n",
      "{'loss': 1.0617, 'learning_rate': 4.804160499645667e-05, 'epoch': 3.42}         \n",
      "{'loss': 1.0507, 'learning_rate': 4.515481287506811e-05, 'epoch': 3.47}         \n",
      "{'loss': 1.0936, 'learning_rate': 4.2331967788513295e-05, 'epoch': 3.53}        \n",
      "{'loss': 1.1391, 'learning_rate': 3.9576361047890554e-05, 'epoch': 3.58}        \n",
      "{'loss': 1.0369, 'learning_rate': 3.689120556739475e-05, 'epoch': 3.63}         \n",
      "{'loss': 1.0486, 'learning_rate': 3.427963211820274e-05, 'epoch': 3.68}         \n",
      "{'loss': 1.0837, 'learning_rate': 3.174468567813461e-05, 'epoch': 3.74}         \n",
      "{'loss': 1.0387, 'learning_rate': 2.9289321881345254e-05, 'epoch': 3.79}        \n",
      "{'loss': 1.0446, 'learning_rate': 2.691640357218759e-05, 'epoch': 3.84}         \n",
      "{'loss': 1.0931, 'learning_rate': 2.4628697467263918e-05, 'epoch': 3.89}        \n",
      "{'loss': 1.0224, 'learning_rate': 2.242887092955801e-05, 'epoch': 3.95}         \n",
      " 79%|█████████████████████████████████▉         | 75/95 [01:47<00:22,  1.11s/it]Invalidate trace cache @ step 315: expected module 4, but got module 3\n",
      "{'loss': 1.0538, 'learning_rate': 2.0319488858409553e-05, 'epoch': 4.0}         \n",
      "{'loss': 1.0334, 'learning_rate': 1.8303010698955804e-05, 'epoch': 4.05}        \n",
      "{'loss': 1.0774, 'learning_rate': 1.638178757452894e-05, 'epoch': 4.11}         \n",
      "{'loss': 1.0557, 'learning_rate': 1.4558059545351143e-05, 'epoch': 4.16}        \n",
      "{'loss': 1.06, 'learning_rate': 1.2833952996724863e-05, 'epoch': 4.21}          \n",
      "{'loss': 1.028, 'learning_rate': 1.1211478159762478e-05, 'epoch': 4.26}         \n",
      "{'loss': 1.0657, 'learning_rate': 9.692526767546729e-06, 'epoch': 4.32}         \n",
      "{'loss': 1.0299, 'learning_rate': 8.278869849454718e-06, 'epoch': 4.37}         \n",
      "{'loss': 1.0354, 'learning_rate': 6.972155666216684e-06, 'epoch': 4.42}         \n",
      "{'loss': 1.0298, 'learning_rate': 5.77390778811796e-06, 'epoch': 4.47}          \n",
      "{'loss': 1.0075, 'learning_rate': 4.685523318583918e-06, 'epoch': 4.53}         \n",
      "{'loss': 1.0469, 'learning_rate': 3.7082712652200867e-06, 'epoch': 4.58}        \n",
      "{'loss': 1.0972, 'learning_rate': 2.843291060205855e-06, 'epoch': 4.63}         \n",
      "{'loss': 1.0568, 'learning_rate': 2.091591231767709e-06, 'epoch': 4.68}         \n",
      "{'loss': 1.0726, 'learning_rate': 1.4540482282803137e-06, 'epoch': 4.74}        \n",
      "{'loss': 1.0477, 'learning_rate': 9.314053963669245e-07, 'epoch': 4.79}         \n",
      "{'loss': 1.096, 'learning_rate': 5.24272114190516e-07, 'epoch': 4.84}           \n",
      "{'loss': 1.0432, 'learning_rate': 2.3312308094607382e-07, 'epoch': 4.89}        \n",
      "{'loss': 1.0599, 'learning_rate': 5.8297763382597626e-08, 'epoch': 4.95}        \n",
      " 99%|██████████████████████████████████████████▌| 94/95 [02:12<00:01,  1.11s/it]Invalidate trace cache @ step 315: expected module 4, but got module 3\n",
      "{'loss': 1.043, 'learning_rate': 0.0, 'epoch': 5.0}                             \n",
      "{'train_runtime': 139.419, 'train_samples_per_second': 10.759, 'train_steps_per_second': 0.681, 'train_loss': 1.1568048044254906, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████████| 95/95 [02:13<00:00,  1.41s/it]\n",
      "[2024-03-07 19:08:41,681] [INFO] [launch.py:348:main] Process 1979528 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!{finetune_script}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbc96599-d922-47e8-b00c-485e2563d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-07 19:38:00,754] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mahsa.geshvadi001/Projects/LLaVA/scripts/merge_lora_weights.py\", line 22, in <module>\n",
      "    merge_lora(args)\n",
      "  File \"/home/mahsa.geshvadi001/Projects/LLaVA/scripts/merge_lora_weights.py\", line 8, in merge_lora\n",
      "    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, device_map='cpu')\n",
      "                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mahsa.geshvadi001/Projects/LLMP_mahsa/LLMP/LLMP/LLaVA/llava/model/builder.py\", line 121, in load_pretrained_model\n",
      "    model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mahsa.geshvadi001/miniconda3/envs/LLMP/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 569, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized configuration class <class 'transformers.models.llava.configuration_llava.LlavaConfig'> for this kind of AutoModel: AutoModelForCausalLM.\n",
      "Model type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MvpConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, LlavaConfig, LlavaMptConfig, LlavaMistralConfig.\n"
     ]
    }
   ],
   "source": [
    "!python scripts/merge_lora_weights.py --model-path lenghtDataset/checkpoints --model-base liuhaotian/llava-v1.5-7b --save-model-path lengthDataset/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c3d95-ee35-42f6-8cf2-4d2e390c8781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33c88b-4e0c-4d60-9761-d71d6e141c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd44f7b-9d14-4814-bb07-da269adbd9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3e4b1-34ec-407a-a770-317ae57479cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
